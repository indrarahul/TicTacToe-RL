{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4_3cFpsmZUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shmnxiy1HQoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def boardtostate(squares):\n",
        "\tid1 = 0\n",
        "\tfor i in range(9):\n",
        "\t\tval = 0\n",
        "\t\tif (squares[i]!='-'):\n",
        "\t\t\tif squares[i] == 'X':\n",
        "\t\t\t\tval = 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tval = 2\n",
        "\t\t\tid1 += val * (3**i)\n",
        "\treturn id1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ8DWG2UHfTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def statetoboard(s):\n",
        "\tstate = s\n",
        "\tboard = ''\n",
        "\tfor i in range(9):\n",
        "\t\tval = int(s/(3**(8-i)))\n",
        "\t\tif val!=0:\n",
        "\t\t\tif val == 1:\n",
        "\t\t\t\tboard += 'X'\t# X => 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tboard += 'O'\t# O => 2\n",
        "\t\t\ts -= (int) (val * (3**(8-i)))\n",
        "\t\telif val==0:\n",
        "\t\t\tboard += '-'\n",
        "\treturn board"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx3wfu74Hh0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Actions(state):\n",
        "\texisting = state\n",
        "\tactions = []\n",
        "\tfor i in range(9):\n",
        "\t\tval = (int) (state/(3**(8-i)))\n",
        "\t\tif val == 0:\n",
        "\t\t\tsp = existing + (int) (2*(3**(8-i)))  #agent is'O' so \"2\"\n",
        "\t\t\tactions.append(sp)\n",
        "\t\tstate-= (int) (val * (3**(8-i)))\n",
        "\treturn actions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ8c3Rp0Hp4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "squares = 'X--OX---O'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aUqlzK6Lm0k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ad34964-0406-4068-a1a7-59e458bb054e"
      },
      "source": [
        "Actions(13728)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[18102, 15186, 13730]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8ajuOjC0KeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_size = 1\n",
        "action_size = np.array(Actions(state)).size\n",
        "\n",
        "max_episodes = 200\n",
        "learning_rate = 0.05\n",
        "gamma = 0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq3hOfBk0dfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discount_and_normalize_rewards(episode_rewards):\n",
        "  discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
        "  cumulative = 0.0\n",
        "  for i in reversed(range(len(episode_rewards))):\n",
        "    cumulative = cumulative * gamma + episode_rewards[i]\n",
        "    discounted_episode_rewards[i] = cumulative\n",
        "  \n",
        "  mean = np.mean(discounted_episode_rewards)\n",
        "  std = np.std(discounted_episode_rewards)\n",
        "  discounted_episode_rewards = (discounted_episode_rewards-mean)/std\n",
        "  \n",
        "  return discounted_episode_rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzG85a0lH-IQ",
        "colab_type": "text"
      },
      "source": [
        "I am creating 3 layers here (Input -> FC1 -> FC2 -> FC3 -> Output)\n",
        "\n",
        "Ouput function is softmax that makes the outputs to a probability distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJZLGsTRIabt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.name_scope(name=\"input\"):\n",
        "  \n",
        "  input_ = tf.placeholder(tf.float32,[None,1],name=\"input\")\n",
        "  actions = tf.placeholder(tf.int32, [None,action_size],name=\"actions\")\n",
        "  discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,],name=\"discounted_episode_rewards_\")\n",
        "  \n",
        "  mean_reward_ = tf.placeholder(tf.float32,name=\"mean_reward\")\n",
        "  \n",
        "  with tf.name_scope(\"FC1\"):\n",
        "    FC1 = tf.contrib.layers.fully_connected(inputs=input_,num_outputs=10,activation_fn=tf.nn.relu,weights_initializer=tf.contrib.layers.xavier_initializer())\n",
        "  \n",
        "  with tf.name_scope(\"FC2\"):\n",
        "    FC2 = tf.contrib.layers.fully_connected(inputs=FC1,num_outputs=action_size,activation_fn=tf.nn.relu,weights_initializer=tf.contrib.layers.xavier_initializer())\n",
        "  \n",
        "  with tf.name_scope(\"FC3\"):\n",
        "    FC3 = tf.contrib.layers.fully_connected(inputs=FC2,num_outputs=action_size,activation_fn=None,weights_initializer=tf.contrib.layers.xavier_initializer())\n",
        "    \n",
        "  with tf.name_scope(\"softmax\"):\n",
        "    action_distribution = tf.nn.softmax(FC3)\n",
        "  \n",
        "  with tf.name_scope(\"loss\"):\n",
        "    neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=FC3,labels=actions)\n",
        "    loss = tf.reduce_mean(neg_log_prob*discounted_episode_rewards_)\n",
        "  \n",
        "  with tf.name_scope(\"train\"):\n",
        "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4WB9T4APllU",
        "colab_type": "text"
      },
      "source": [
        "As we had to maximize the Rewards but here we are using gradient descent algorithm because we have taken negative of log , so minimizing this function will actually maximize the rewards that is gradient ascent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtFREhCsP1I-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allRewards = []\n",
        "total_rewards = 0\n",
        "maximumRewardRecorded = 0\n",
        "episode = 0\n",
        "episode_states, episode_actions, episode_rewards = [],[],[]\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  for episode in range(max_episodes):\n",
        "    episode_rewards_sum = 0\n",
        "    state = 0\n",
        "    while True:\n",
        "      action_probability_distribution = sess.run(action_distribution,feed_dict={input_:state.reshape([1,1])})\n",
        "      action = np.random.choice(range(action_probability_distribution.shape[1]),p= action_probability_distribution.ravel())   # p= probability array and .ravel() -> to contiguous flatten the array\n",
        "      \n",
        "      #perform the action\n",
        "      new_state, reward, performed = ### to implement    After performing the action on environment it should return new_state-> int, reward -> int, performed -> bool\n",
        "      \n",
        "      episode_states.append(state)\n",
        "      action_ = np.zeros(action_size)\n",
        "      action_[action] = 1\n",
        "      episode_actions.append(action_)\n",
        "      episode_rewards.append(reward)\n",
        "      \n",
        "      if performed: #If action performed\n",
        "        episode_rewards_sum = np.sum(episode_rewards)\n",
        "        allRewards.append(episode_rewards_sum)\n",
        "        total_rewards = np.sum(allRewards)\n",
        "        mean_reward = np.divide(total_rewards,episode+1)\n",
        "        maximumRewardRecorded = np.amax(allRewards)\n",
        "        print(\"**************\")\n",
        "        print(\"Episode:\",episode)\n",
        "        print(\"Reward: \",episode_rewards_sum)\n",
        "        print(\"Mean Reward: \", mean_reward)\n",
        "        print(\"Max Reward so far: \", maximumRewardRecorded)\n",
        "        print(\"**************\")\n",
        "        \n",
        "        discounted_episode_rewards = discount_and_normalize_rewards(episode_rewards)\n",
        "        loss_,_ = sess.run([loss,train_opt],feed_dict={input_: np.vstack(np.array(episode_states)),\n",
        "                                                       actions: np.vstack(np.array(episode_actions)),\n",
        "                                                       discounted_episode_rewards_: discounted_episode_rewards \n",
        "                                                      })\n",
        "        summary = sess.run(write_op, feed_dict={input_: np.vstack(np.array(episode_states)),\n",
        "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
        "                                                                 discounted_episode_rewards_: discounted_episode_rewards,\n",
        "                                                                    mean_reward_: mean_reward\n",
        "                                                                })\n",
        "                \n",
        "               \n",
        "        writer.add_summary(summary, episode)\n",
        "        writer.flush()\n",
        "        episode_states, episode_actions, episode_rewards = [],[],[]\n",
        "        break\n",
        "    state = new_state\n",
        "  \n",
        "  if episode % 50 == 0:\n",
        "    saver.save(sess,\".models/model.ckpt\")\n",
        "    print(\"SAVED\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}